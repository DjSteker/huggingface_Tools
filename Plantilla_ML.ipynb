{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "K7fVc7dsM_tj",
        "27HAmiIZNV6C",
        "WQ6Vm-2jS0S6"
      ],
      "authorship_tag": "ABX9TyMj/7jnmaemfShN3FViw9ZI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DjSteker/huggingface_Tools/blob/main/Plantilla_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Instalar"
      ],
      "metadata": {
        "id": "CCJU6ZeNMLSx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KS3ta0NkMKNo"
      },
      "outputs": [],
      "source": [
        "pip install transformers\n",
        "pip install datasets\n",
        "pip install evaluate\n",
        "pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers evaluate datasets accelerate -q"
      ],
      "metadata": {
        "id": "V0usI6tHMiY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___________________"
      ],
      "metadata": {
        "id": "rf2rlulMMzGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Inicializar"
      ],
      "metadata": {
        "id": "K7fVc7dsM_tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "Jx_R2otJM02e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"openai-community/gpt2\" # https://huggingface.co/openai-community/gpt2\n"
      ],
      "metadata": {
        "id": "KCVshUTYNJL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=\"bert-base-uncased\" #https://www.philschmid.de/pre-training-bert-habana\n"
      ],
      "metadata": {
        "id": "VRpRiNTENKOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=\"tloen/alpaca-lora-7b\" # https://huggingface.co/tloen/alpaca-lora-7b\n"
      ],
      "metadata": {
        "id": "Op5KI0CXNKwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6AcJIH3dOoD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "____________________________"
      ],
      "metadata": {
        "id": "osTTrlAaNUdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instancias"
      ],
      "metadata": {
        "id": "27HAmiIZNV6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import list_datasets, load_dataset, DatasetInfo\n",
        "from huggingface_hub import list_datasets, dataset_info\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import pipeline\n",
        "from transformers import  BertConfig, BertModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import list_datasets, load_dataset, DatasetInfo\n",
        "from transformers import Trainer, TrainingArguments\n"
      ],
      "metadata": {
        "id": "CY1pvusPNc0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instancias Separadas"
      ],
      "metadata": {
        "id": "b40MtbNVNiKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "AwbjaMY2OHKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import list_datasets, dataset_info"
      ],
      "metadata": {
        "id": "5pU7ITuLNoX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import list_datasets, load_dataset, DatasetInfo"
      ],
      "metadata": {
        "id": "n_rDDJPcNu2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "yWjsnh10N1oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "F2mbOnejN4UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "9mAFrCBsN7wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "NdCrwOwaN_7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import  BertConfig, BertModel"
      ],
      "metadata": {
        "id": "5wXBT3QQOMrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import list_datasets, load_dataset, DatasetInfo"
      ],
      "metadata": {
        "id": "1YDeOX2WOT8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ea4s4sAmPKPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n"
      ],
      "metadata": {
        "id": "ziAGZ2wAOYia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_________________"
      ],
      "metadata": {
        "id": "CrB4lGO1PBaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cargar dataset"
      ],
      "metadata": {
        "id": "yzk5lhBnPE7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_datasets = list_datasets()\n",
        "print(f\"Existen {len(all_datasets)} datasets\")\n",
        "all_datasets = list_datasets(sort=\"downloads\", direction=-1, limit=5)"
      ],
      "metadata": {
        "id": "omSs8NXQPLZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"ecastera/filosofia-es\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "n6xjZxW7TPYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"yelp_review_full\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "BXQwKZh3PePs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"hackathon-somos-nlp-2023/winogrande_train_s_spanish\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "0HNR52ZEPeKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"huggingface/CodeBERTa-small-v1\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "JCvYwbGePeEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"matlok/multimodal-python-copilot-training-overview\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "XGyLLorMPd8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"hackathon-somos-nlp-2023/Habilidades_Agente_v1\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "ara5Ig5nPdwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "el_quijote = load_dataset(\"text\", data_files=\"https://mymldatasets.s3.eu-de.cloud-object-storage.appdomain.cloud/el_quijote.txt\")\n",
        "el_quijote\n",
        "el_quijote.set_format(type='pandas')\n",
        "df = el_quijote['train'][:]\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "Jtz2K0jMQu-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = dataset"
      ],
      "metadata": {
        "id": "zOKpCzVERdIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__________"
      ],
      "metadata": {
        "id": "-RUXFtXUQkf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "atributos = dataset.column_names\n",
        "\n",
        "# Imprimir los atributos\n",
        "print(atributos)"
      ],
      "metadata": {
        "id": "iDNnAZeRRlKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1umFTixwRnSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__________"
      ],
      "metadata": {
        "id": "IoaA_86aRmMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.save_to_disk(\"./home/winogrande_train_s_spanish1\")"
      ],
      "metadata": {
        "id": "40Eje9doQjpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.push_to_hub(\"DjSteker/\")"
      ],
      "metadata": {
        "id": "9AMQXlHNPdJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___________________________"
      ],
      "metadata": {
        "id": "DS5IjZqSQ_2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparar"
      ],
      "metadata": {
        "id": "w8pAmzWvRBac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(modelo)"
      ],
      "metadata": {
        "id": "SXJI0z0nRF8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = \"saved\"\n",
        "tokenizer.save_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "AXD1H52xRNQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____"
      ],
      "metadata": {
        "id": "wRV9kdZNRT5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = el_quijote['train'][:]\n",
        "df['name'].value_counts().plot(kind='barh')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ya9B87F-RVJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___________________"
      ],
      "metadata": {
        "id": "bktkCzMIRrns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "rellenar"
      ],
      "metadata": {
        "id": "3mTXiT7iRue6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# para un mejor entrenamiento utilizar más datos\n",
        "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(18119))\n",
        "#small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(51941))\n",
        "#small_eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(25971))\n",
        "small_eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(25971))"
      ],
      "metadata": {
        "id": "j2OumJoVRwaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(modelo)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n",
        "small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "SBZ7rd22R1Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(modelo, num_labels=5)"
      ],
      "metadata": {
        "id": "2oDyr4gQR8fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"accuracy\")\n",
        "# calculamos el accuracy de las predicciones\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "HIbm7u85R_ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# definimos los argumentos del entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    'mi-super-modelo',\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_steps=5,\n",
        "    num_train_epochs=1,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "# instanciamos el objeto Trainer con todo lo que hemos preparado\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "# 8 min con 80 ejemplos (aprox)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "jSvUtTCtSGs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\"test-trainer\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.traing()"
      ],
      "metadata": {
        "id": "rd1_VNXfSsBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Entrenador 0"
      ],
      "metadata": {
        "id": "9RBnLO6dTdpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "modelo = \"DjSteker/mi-super-modelo\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelo)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n",
        "small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(modelo, num_labels=5)\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "# calculamos el accuracy de las predicciones\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# definimos los argumentos del entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    'mi-super-modelo',\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_steps=5,\n",
        "    num_train_epochs=1,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "# instanciamos el objeto Trainer con todo lo que hemos preparado\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "5TybLDfHTcHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eC4_YHcvT6RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Entrenador 1"
      ],
      "metadata": {
        "id": "WQ6Vm-2jS0S6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import list_datasets, load_dataset, DatasetInfo\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Cargar el dataset\n",
        "el_quijote = load_dataset(\"text\", data_files=\"https://mymldatasets.s3.eu-de.cloud-object-storage.appdomain.cloud/el_quijote.txt\")\n",
        "train_data = el_quijote['train']\n",
        "\n",
        "\n",
        "el_quijote.set_format(type='pandas')\n",
        "df_data = dataset['train'][:] #el_quijote['train'][:]\n",
        "#df_data.head()\n",
        "\n",
        "# Preprocesar el texto\n",
        "tokenizer =BertTokenizer.from_pretrained(model) #BertTokenizer.from_pretrained('DjSteker/mi-super-modelo2')\n",
        "max_length = 256\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx]['text']\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True\n",
        "        )\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Crear los dataloaders\n",
        "batch_size = 8\n",
        "train_dataset = CustomDataset(df_data, tokenizer, max_length)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Configurar el dispositivo\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Crear el modelo y el optimizador\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "num_epochs = 10\n",
        "total_steps = len(train_loader) * num_epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')"
      ],
      "metadata": {
        "id": "Ch2amh-cS4Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____"
      ],
      "metadata": {
        "id": "F5B1zuauSOWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardar modelo"
      ],
      "metadata": {
        "id": "HWmPdy3dSPbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./modelo1\")"
      ],
      "metadata": {
        "id": "eaGzar5vSQX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "hdo9nJmjSU7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(\"DjSteker/mi-super-modelo\")"
      ],
      "metadata": {
        "id": "NW_a8I2bSXKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.push_to_hub(\"DjSteker/yelp_review_full1\")"
      ],
      "metadata": {
        "id": "jZpaTee9SaT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.save_to_disk(yelp_review_full1)"
      ],
      "metadata": {
        "id": "TOn-EqxPScYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_________________________"
      ],
      "metadata": {
        "id": "nmslJu5ES6rA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Jugar"
      ],
      "metadata": {
        "id": "yRh28QvBS8Yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "#summarizer = pipeline(\"summarization\", model=\"sshleifer/distibart-cnn-12-6\")\n",
        "summarizer = pipeline(\"summarization\", model)\n",
        "\n",
        "text = \"Javier, un hombre con una camisa a cuadros y calcetines de colores, lucha con un cuadro torcido en la pared. Carlos, su amigo, entra con una caja de pizza en la mano.\"\n",
        "\n",
        "print(summarizer(text, max_length=130, min_length=30, dosample=False))"
      ],
      "metadata": {
        "id": "QhplXM54TAQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = generator(\"hola ¿ cómo te llamas ?\" , max_length=30, num_return_sequences=4,)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "3qgl0KV4TBEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLA6-jVpTAyH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}